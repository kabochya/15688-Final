{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flight Delay Prediction\n",
    "## Introduction\n",
    "In our project, we try to predict whether a flight would delay or not with local weather observations. We also try to predict which 15-minute interval the arrival of a flight falls into. We chose the flight route from the 3 major  airports around New York City and landing at two larger airports in Miami. This flight route was chosen because, first, they are in the same time zone (which solved some problems that would be discussed later). Second, the amount of flights were large enough, more than 70 flights per day in average. Third, the airports in New York City are notorious for there poor on-time performance, especially when taking weather into accound. These reasons made it seem wise to start our project from this route, although other routes might also work as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements\n",
    "```\n",
    "pip3 install numpy pandas lxml bs4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "### Flight Data\n",
    "We get our flight data from the [Bureau of Transportation Statistics](https://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&DB_Short_Name=On-Time). The database itself is not program-friendly, you'll have to fetch the data manually to get the data. But after looking into the requests, we found out that the form data sent along with the request is actually a raw SQL query (which is quite dangerous in a public site), and we can get the data needed in a single request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching data from BTS database\n",
    "\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "def get_data_csv():\n",
    "    years_interval = [\"2015\",\"2018\"]\n",
    "    req_header = {\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Accept-Language\": \"zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "        \"Cache-Control\": \"no-cache\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "        \"Cookie\": \"ASPSESSIONIDCQCDRTCQ=FKGMOCEDMCDHINELAEGCEIKN\",\n",
    "        \"Host\": \"www.transtats.bts.gov\",\n",
    "        \"Origin\": \"https://www.transtats.bts.gov\",\n",
    "        \"Pragma\": \"no-cache\",\n",
    "        \"Referer\": \"https://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&DB_Short_Name=On-Time\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.139 Safari/537.36\"\n",
    "    }\n",
    "    data_url = \"https://www.transtats.bts.gov/DownLoad_Table.asp?Table_ID=236&Has_Group=3&Is_Zipped=0\"\n",
    "    pair_form_data = {\n",
    "    \"UserTableName\":\"On_Time_Performance\",\n",
    "    \"DBShortName\":\"On_Time\",\n",
    "    \"RawDataTable\":\"T_ONTIME\",\n",
    "    \"sqlstr\":\\\n",
    "            \"SELECT YEAR,QUARTER,MONTH,DAY_OF_MONTH,DAY_OF_WEEK,FL_DATE,CARRIER,TAIL_NUM,ORIGIN,DEST,CRS_DEP_TIME,DEP_TIME,DEP_DELAY,CRS_ARR_TIME,ARR_TIME,ARR_DELAY,ARR_DELAY_GROUP,DISTANCE,CANCELLED,DIVERTED,CARRIER_DELAY,WEATHER_DELAY,NAS_DELAY,SECURITY_DELAY,LATE_AIRCRAFT_DELAY \\\n",
    "            FROM T_ONTIME \\\n",
    "            WHERE (\\\n",
    "                DEST IN ('EWR','JFK','LGA','MIA','FLL') OR\\\n",
    "                ORIGIN IN ('EWR','JFK','LGA','MIA','FLL')\\\n",
    "            )\\\n",
    "            AND \\\n",
    "            Month BETWEEN 1 AND 12 AND \\\n",
    "            YEAR BETWEEN {} AND {}\".format(years_interval[0],years_interval[1]),\n",
    "    \"varlist\":\"YEAR,QUARTER,MONTH,DAY_OF_MONTH,DAY_OF_WEEK,FL_DATE,CARRIER,TAIL_NUM,ORIGIN,DEST,CRS_DEP_TIME,DEP_TIME,DEP_DELAY,CRS_ARR_TIME,ARR_TIME,ARR_DELAY,ARR_DELAY_GROUP,DISTANCE,CANCELLED,DIVERTED,CARRIER_DELAY,WEATHER_DELAY,NAS_DELAY,SECURITY_DELAY,LATE_AIRCRAFT_DELAY\",\n",
    "    }\n",
    "    r = requests.post(data_url,\n",
    "                      data=pair_form_data,\n",
    "                      headers=req_header,\n",
    "                     )\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    return pd.read_csv(io.BytesIO(z.read(z.infolist()[0])),\n",
    "                       dtype={\n",
    "                          \"CRS_DEP_TIME\":str,\n",
    "                          \"DEP_TIME\":str,\n",
    "                          \"CRS_ARR_TIME\":str,\n",
    "                          \"ARR_TIME\":str,\n",
    "                          \"CANCELLED\":bool,\n",
    "                          \"DIVERTED\":bool},\n",
    "                      usecols=list(range(24))\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Data\n",
    "In the beginning, we thought that historical weather data is easy to find. Turns out it's a bit harder than we thought to obtain free weather data in such a fine-grain scale. We did some extra research and [www.wunderground.com](Weather Underground) had statically rendered hourly historical weather data pages, but the API was not public. We had to parse the weather data with BeautifulSoup. Since we have 5 airports spanning out a bit over 3 years, it took some time for us to parse all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping hourly weather data from wunderground.com\n",
    "\n",
    "import lxml\n",
    "from multiprocessing.dummy import Pool\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "NYC_AIRPORTS = [\"EWR\",\"JFK\",\"LGA\"]\n",
    "MIA_AIRPORTS = [\"FLL\",\"MIA\"]\n",
    "\n",
    "def wind_speed_conv(x):\n",
    "    try:\n",
    "        ret = 0.0 if x == \"Calm\" else float(x)\n",
    "    except:\n",
    "        ret = np.NaN\n",
    "    return ret\n",
    "DIR_ANGLE={\n",
    "            \"North\": 0,\"NNE\": 1,\"NE\": 2,\"ENE\": 3,\"East\": 4,\"ESE\": 5,\"SE\": 6,\"SSE\": 7,\n",
    "            \"South\": 8,\"SSW\": 9,\"SW\":10,\"WSW\":11,\"West\":12,\"WNW\":13,\"NW\":14,\"NNW\":15,\n",
    "          }\n",
    "def wind_dir_conv(x):\n",
    "    return np.NaN if x not in DIR_ANGLE else 22.5*DIR_ANGLE[x]\n",
    "def humidity_conv(x):\n",
    "    try:\n",
    "        ret = float(x.strip(\"%\"))/100\n",
    "    except:\n",
    "        ret = np.NaN\n",
    "    return ret\n",
    "def precip_conv(x):\n",
    "    try:\n",
    "        ret = 0.0 if x == \"N/A\" else float(x)\n",
    "    except:\n",
    "        ret = 0.0\n",
    "    return ret\n",
    "def event_conv(x):\n",
    "    return \"\".join(x.split(\"\\t\"))\n",
    "\n",
    "\n",
    "def table_parser(pair):\n",
    "    url, date, airport = pair\n",
    "    content = requests.get(url).content\n",
    "    def time_conv(x):\n",
    "        return date + \" \" + x\n",
    "    bs = BeautifulSoup(content, \"html5lib\")\n",
    "    table_html = bs.find(id=\"obsTable\")\n",
    "    for unit_tag in table_html.find_all(\"span\",class_=\"wx-unit\"):\n",
    "        unit_tag.decompose()\n",
    "    table_df = pd.read_html(str(table_html), converters= \\\n",
    "                            {\n",
    "                                \"Time (EST)\":time_conv,\n",
    "                                \"Time (EDT)\":time_conv,\n",
    "                                \"Wind Speed\":wind_speed_conv,\n",
    "                                \"Wind Dir\":wind_dir_conv,\n",
    "                                \"Precip\":precip_conv,\n",
    "                                \"Humidity\":humidity_conv,\n",
    "                                \"Events\":event_conv\n",
    "                            })[0]\n",
    "    table_df = table_df.drop([\"Windchill\", \"Dew Point\", \"Gust Speed\"],axis=1, errors=\"ignore\")\n",
    "    table_df.rename(columns={\"Time (EST)\":\"Time\", \"Time (EDT)\":\"Time\", \"Temp.\":\"Temp\"}, inplace=True)\n",
    "    table_df[\"Time\"] = pd.to_datetime(table_df[\"Time\"])\n",
    "    table_df.set_index(\"Time\", inplace=True)\n",
    "    table_df[\"Airport\"] = airport\n",
    "    return table_df\n",
    "\n",
    "def weather_scraper(dates):\n",
    "    # Run once. Slow to run.\n",
    "    base_url = \"https://www.wunderground.com/history/airport/K{}/{}/DailyHistory.html\"\n",
    "    \n",
    "    pool = Pool(8)\n",
    "    pairs = []\n",
    "    for airport in NYC_AIRPORTS + MIA_AIRPORTS:\n",
    "        for date in dates:\n",
    "            date_str = date.strftime(\"%Y/%m/%d\")\n",
    "            url = base_url.format(airport, date_str)\n",
    "            pairs.append((url, date_str, airport))\n",
    "            \n",
    "    weather_df_list = pool.map(table_parser, pairs)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "            \n",
    "    result = pd.concat(weather_df_list)\n",
    "    result.to_csv(\"weather.csv\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv(\"weather.csv\", low_memory=False, parse_dates=[\"Time\"], na_values=[\"-\"], index_col=\"Time\")\n",
    "weather = weather.drop(\"Heat Index\",axis=1)\n",
    "\n",
    "condition_dict={\n",
    "    \"Clear\":\"Clear\",\n",
    "    \"Mostly Cloudy\":\"Mostly Cloudy\",\n",
    "    \"Partly Cloudy\":\"Partly Cloudy\",\n",
    "    \"Scattered Clouds\":\"Scattered Clouds\",\n",
    "    \"Overcast\":\"Overcast\",\n",
    "    \"Light Snow\":\"Light Snow\",\n",
    "    \"Light Rain\":\"Light Rain\",\n",
    "    \"Snow\":\"Snow\",\n",
    "    \"Light Drizzle\":\"Light Drizzle\",\n",
    "    \"Rain\":\"Rain\",\n",
    "    \"Fog\":\"Fog\",\n",
    "    \"Light Freezing Drizzle\":\"Light Freezing Drizzle\",\n",
    "    \"Light Freezing Rain\":\"Light Freezing Rain\",\n",
    "    \"Heavy Rain\":\"Heavy Rain\",\n",
    "    \"Thunderstorms and Rain\":\"Thunderstorm|Rain\",\n",
    "    \"Heavy Snow\":\"Heavy Snow\",\n",
    "    \"Light Ice Pellets\":\"Light Ice Pellets\",\n",
    "    \"Blowing Snow\":\"Blowing Snow\",\n",
    "    \"Haze\":\"Haze\",\n",
    "    \"Ice Pellets\":\"Ice Pellets\",\n",
    "    \"Light Thunderstorms and Rain\":\"Light Rain|Thunderstorm\",\n",
    "    \"Heavy Thunderstorms and Rain\":\"Heavy Rain|Thunderstorm\",\n",
    "    \"Thunderstorm\":\"Thunderstorm\",\n",
    "    \"Mist\":\"Mist\",\n",
    "    \"Light Rain Showers\":\"Light Rain Showers\",\n",
    "    \"Unknown\":\"Unknown\",\n",
    "    \"Shallow Fog\":\"Shallow Fog\",\n",
    "    \"Heavy Ice Pellets\":\"Heavy Ice Pellets\",\n",
    "    \"Heavy Thunderstorms with Ice Pellets\":\"Heavy Ice Pellets|Thunderstorm\",\n",
    "    \"Squalls\":\"Squalls\",\n",
    "    \"Patches of Fog\":\"Patches of Fog\",\n",
    "    \"Smoke\":\"Smoke\",\n",
    "    \"Light Freezing Fog\":\"Light Freezing Fog\",\n",
    "    \"Funnel Cloud\":\"Funnel Cloud\"\n",
    "}\n",
    "weather[\"Conditions\"] = weather[\"Conditions\"].apply(lambda x: condition_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle weather with DST\n",
    "new_weather_list = []\n",
    "for a in NYC_AIRPORTS+MIA_AIRPORTS:\n",
    "    weather_airport_idx = weather[weather[\"Airport\"]==a].index\n",
    "    weather_airport_tz_idx = weather_airport_idx.tz_localize('US/Eastern', ambiguous='infer')\n",
    "    new_weather = weather[weather[\"Airport\"]==a].set_index(weather_airport_tz_idx)\n",
    "    new_weather_list.append(new_weather[~new_weather.index.duplicated()])\n",
    "new_weather = pd.concat(new_weather_list)\n",
    "    \n",
    "new_weather.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment these lines to download the flight data\n",
    "# raw_data = get_data_csv()\n",
    "# raw_data.to_csv(\"data.csv\", index=False)\n",
    "raw_data = pd.read_csv(\"data.csv\",dtype={\n",
    "                          \"CRS_DEP_TIME\":str,\n",
    "                          \"DEP_TIME\":str,\n",
    "                          \"CRS_ARR_TIME\":str,\n",
    "                          \"ARR_TIME\":str,\n",
    "                          \"CANCELLED\":bool,\n",
    "                          \"DIVERTED\":bool})\n",
    "raw_data[[\"DEP_DELAY\",\"ARR_DELAY\"]] = raw_data[[\"DEP_DELAY\",\"ARR_DELAY\"]].fillna(0)\n",
    "\n",
    "# Uncomment these lines to download weather data\n",
    "# weather_scraper(raw_data.index.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = raw_data[~(raw_data[\"DIVERTED\"] | raw_data[\"CANCELLED\"])].drop([\"CANCELLED\",\"DIVERTED\"], axis=1)\n",
    "\n",
    "time_cols = [\"CRS_DEP_TIME\", \"CRS_ARR_TIME\"]\n",
    "for col in time_cols:\n",
    "    data.loc[data[col]==\"2400\",col] = \"0000\"\n",
    "    data[col.rsplit(\"_\",1)[0]+\"_DATETIME\"] = \\\n",
    "    pd.to_datetime(data[\"FL_DATE\"]+\" \"+data[col], format=\"%Y-%m-%d %H%M\")\n",
    "    \n",
    "# check if arrival time is after departure\n",
    "data.loc[(data['CRS_ARR_DATETIME'] < data['CRS_DEP_DATETIME']), 'CRS_ARR_DATETIME'] += pd.Timedelta(days=1)\n",
    "data['CRS_DEP_DATETIME'] = pd.DatetimeIndex(data[\"CRS_DEP_DATETIME\"]).tz_localize(\"US/Eastern\", ambiguous=False, errors=\"coerce\")\n",
    "data['CRS_ARR_DATETIME'] = pd.DatetimeIndex(data[\"CRS_ARR_DATETIME\"]).tz_localize(\"US/Eastern\", ambiguous=False, errors=\"coerce\")\n",
    "\n",
    "data[\"DEP_DATETIME\"] = data[\"CRS_DEP_DATETIME\"] + pd.to_timedelta(data[\"DEP_DELAY\"], unit=\"m\")\n",
    "data[\"ARR_DATETIME\"] = data[\"CRS_ARR_DATETIME\"] + pd.to_timedelta(data[\"ARR_DELAY\"], unit=\"m\")\n",
    "data = data[~(data[[\"DEP_DATETIME\",\"ARR_DATETIME\"]].isnull().any(axis=1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mia_data = data.drop(time_cols+[\"DEP_TIME\", \"ARR_TIME\"],axis=1)\n",
    "mia_data = mia_data[mia_data[\"DEST\"].isin(MIA_AIRPORTS) & mia_data[\"ORIGIN\"].isin(NYC_AIRPORTS)]\n",
    "mia_data[\"date\"] = pd.to_datetime(mia_data[\"FL_DATE\"])\n",
    "mia_data = mia_data.set_index(\"date\").sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.merge_asof(mia_data.sort_values(\"CRS_DEP_DATETIME\"), new_weather, left_by=[\"ORIGIN\"], right_by=[\"Airport\"], left_on=\"CRS_DEP_DATETIME\", right_index=True)\n",
    "weather_flight_data = pd.merge_asof(temp_df.sort_values(\"CRS_ARR_DATETIME\"), new_weather, left_by=[\"DEST\"], right_by=[\"Airport\"], left_on=\"CRS_ARR_DATETIME\", right_index=True, suffixes=[\"_dep\",\"_arr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_flight_data = pd.merge_asof(weather_flight_data.sort_values(\"CRS_DEP_DATETIME\"),\n",
    "                                    data[[\"ARR_DATETIME\",\"TAIL_NUM\",\"DEST\",\"ARR_DELAY\"]].sort_values(\"ARR_DATETIME\"),\n",
    "                                    left_by=[\"TAIL_NUM\",\"ORIGIN\"],\n",
    "                                    right_by=[\"TAIL_NUM\",\"DEST\"],\n",
    "                                    left_on=\"CRS_DEP_DATETIME\",\n",
    "                                    right_on=\"ARR_DATETIME\",\n",
    "                                    suffixes=(\"\",\"_PREV\"),\n",
    "                                    allow_exact_matches=False)\n",
    "weather_flight_data[\"PREV_ARR_LT_ONE_DAY\"] = (weather_flight_data[\"CRS_DEP_DATETIME\"] - weather_flight_data[\"ARR_DATETIME_PREV\"] < pd.Timedelta(days=1)).astype(int)\n",
    "weather_flight_data[\"PREV_ARR_LT_SIX_HOURS\"] = (weather_flight_data[\"CRS_DEP_DATETIME\"] - weather_flight_data[\"ARR_DATETIME_PREV\"] < pd.Timedelta(hours=6)).astype(int)\n",
    "weather_flight_data.drop([\"ARR_DATETIME_PREV\", \"DEST_PREV\"],axis=1, inplace=True)\n",
    "weather_flight_data[\"ARR_DELAY_PREV\"] = weather_flight_data[\"ARR_DELAY_PREV\"].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_interval_list = []\n",
    "interval_len = \"30T\"\n",
    "for a in NYC_AIRPORTS+MIA_AIRPORTS:\n",
    "    takeoff = pd.DataFrame(index=data.loc[data[\"ORIGIN\"]==a, \"CRS_DEP_DATETIME\"])\n",
    "    takeoff[\"TAKEOFF_INTV\"] = 1\n",
    "    landing = pd.DataFrame(index=data.loc[data[\"DEST\"]==a, \"CRS_ARR_DATETIME\"])\n",
    "    landing[\"LANDING_INTV\"] = 1\n",
    "    airport_flight_times = takeoff.join(landing,how=\"outer\").fillna(0).groupby(level=0).sum()\n",
    "    rev_aft_idx = pd.datetime(1970,1,1,0,0)+(-(airport_flight_times.index[::-1]-airport_flight_times.index[-1]))\n",
    "    airport_flights_interval = airport_flight_times.rolling(interval_len,closed=\"left\").sum().fillna(0)+\\\n",
    "                          airport_flight_times[::-1].set_index(rev_aft_idx).rolling(interval_len, closed=\"both\").sum()[::-1].set_index(airport_flight_times.index)\n",
    "    airport_flights_interval[\"TOTAL_INTV\"] = airport_flights_interval.sum(axis=1)\n",
    "    airport_flights_interval[\"AIRPORT\"] = a\n",
    "    flights_interval_list.append(airport_flights_interval)\n",
    "flights_interval_df = pd.concat(flights_interval_list).reset_index().rename(index=str,columns={\"index\":\"DATETIME\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = weather_flight_data.merge(flights_interval_df, left_on=[\"ORIGIN\",\"CRS_DEP_DATETIME\"],right_on=[\"AIRPORT\",\"DATETIME\"]).drop([\"AIRPORT\",\"DATETIME\"],axis=1)\n",
    "weather_flight_data = temp_df.merge(flights_interval_df, left_on=[\"DEST\",\"CRS_ARR_DATETIME\"],right_on=[\"AIRPORT\",\"DATETIME\"],suffixes=(\"_ORIGIN\",\"_DEST\")).drop([\"AIRPORT\",\"DATETIME\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_flight_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weather_flight_data\n",
    "carrier_prob = weather_flight_data[['CARRIER','ARR_DELAY']]\\\n",
    "                .groupby(['CARRIER'], group_keys=False)\\\n",
    "                .apply(lambda x: x[x['ARR_DELAY']>0].shape[0]/x.shape[0])\n",
    "ax = carrier_prob.plot(kind='bar', title =\"Delay probability vs Carrier\", figsize=(15, 10), legend=True, fontsize=12)\n",
    "ax.set_xlabel(\"Carrier\", fontsize=12)\n",
    "ax.set_ylabel(\"Delay Prob\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Conditions_arr_prob = weather_flight_data[['Conditions_dep','ARR_DELAY']]\\\n",
    "                .groupby(['Conditions_dep'], group_keys=False)\\\n",
    "                .apply(lambda x: x[x['ARR_DELAY']>0].shape[0]/x.shape[0])\n",
    "ax = Conditions_arr_prob.plot(kind='bar', title =\"Delay Probability vs Weather Conditions\", figsize=(15, 10), legend=True, fontsize=12)\n",
    "ax.set_xlabel(\"Weather Conditions\", fontsize=12)\n",
    "ax.set_ylabel(\"Delay Prob\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "def normalize(feature_list,feature_df):\n",
    "    \n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    \n",
    "    for f in feature_list:        \n",
    "        x_scaled = min_max_scaler.fit_transform(feature_df[[f]].values.astype(float))\n",
    "        feature_df[f]= pd.DataFrame(x_scaled)\n",
    "        \n",
    "    return feature_df\n",
    "\n",
    "\n",
    "features = ['MONTH', 'DAY_OF_WEEK', 'CARRIER', 'ORIGIN',\n",
    "            'DEST', 'DEP_DELAY','ARR_DELAY_GROUP', 'ARR_DELAY', 'CRS_ARR_DATETIME',\n",
    "            'CRS_DEP_DATETIME', 'Conditions_dep', 'Humidity_dep',\n",
    "            'Precip_dep', 'Pressure_dep', 'Temp_dep', 'Visibility_dep',\n",
    "            'Wind Dir_dep', 'Wind Speed_dep',  'Conditions_arr',\n",
    "            'Humidity_arr', 'Precip_arr', 'Pressure_arr', 'Temp_arr',\n",
    "            'Visibility_arr', 'Wind Dir_arr', 'Wind Speed_arr',\n",
    "            'ARR_DELAY_PREV','PREV_ARR_LT_ONE_DAY','PREV_ARR_LT_SIX_HOURS',\n",
    "            'TAKEOFF_INTV_ORIGIN','LANDING_INTV_ORIGIN','TAKEOFF_INTV_DEST','LANDING_INTV_DEST',\n",
    "            'TOTAL_INTV_ORIGIN','TOTAL_INTV_DEST']\n",
    "#['Month','Carrier','origin','Dest','distance']\n",
    "all_df = weather_flight_data[features]\n",
    "all_df = all_df.dropna(axis=0, how='any', subset=set(all_df.columns) - set([\"Wind Dir_arr\", \"Wind Dir_dep\"]))\n",
    "y_df = all_df['ARR_DELAY'].reset_index(drop=True)\n",
    "all_df = all_df.drop(['ARR_DELAY_GROUP','ARR_DELAY'],axis=1).reset_index(drop=True)\n",
    "\n",
    "def sincos(df,feature_list,freq_list):\n",
    "    for feature, freq in zip(feature_list, freq_list):\n",
    "        df[feature+\"_SIN\"] = np.sin(df[feature]/freq*2*np.pi).fillna(0)\n",
    "        df[feature+\"_COS\"] = np.cos(df[feature]/freq*2*np.pi).fillna(0)\n",
    "        \n",
    "sincos(all_df,[\"MONTH\", \"DAY_OF_WEEK\", \"Wind Dir_arr\", \"Wind Dir_dep\"],[12, 7, 360, 360])\n",
    "all_df = all_df.drop([\"MONTH\",\"DAY_OF_WEEK\", \"Wind Dir_arr\", \"Wind Dir_dep\"], axis=1)\n",
    "\n",
    "norm_list = ['Pressure_dep','Pressure_arr','Temp_arr','Temp_dep','Visibility_dep','Visibility_arr',\n",
    "             \"TAKEOFF_INTV_ORIGIN\",\"LANDING_INTV_ORIGIN\",\"TAKEOFF_INTV_DEST\",\n",
    "             \"LANDING_INTV_DEST\",\"TOTAL_INTV_ORIGIN\",\"TOTAL_INTV_DEST\"]\n",
    "feature_df = pd.get_dummies(all_df.drop([\"Conditions_dep\",\"Conditions_arr\"],axis=1))\n",
    "feature_df = normalize(norm_list,feature_df)\n",
    "feature_df = pd.concat([\n",
    "        feature_df,\n",
    "        all_df[\"Conditions_dep\"].str.get_dummies().add_prefix(\"Conditions_dep_\"),\n",
    "        all_df[\"Conditions_arr\"].str.get_dummies().add_prefix(\"Conditions_arr_\")\n",
    "    ],\n",
    "    axis=1)\n",
    "\n",
    "feature_df['Wind_arr_SIN'] = feature_df['Wind Dir_arr_SIN'] * feature_df['Wind Speed_arr']\n",
    "feature_df['Wind_arr_COS'] = feature_df['Wind Dir_arr_COS'] * feature_df['Wind Speed_arr']\n",
    "feature_df['Wind_dep_SIN'] = feature_df['Wind Dir_dep_SIN'] * feature_df['Wind Speed_dep']\n",
    "feature_df['Wind_dep_COS'] = feature_df['Wind Dir_dep_COS'] * feature_df['Wind Speed_dep']\n",
    "feature_df['CRS_ARR_HOUR_SIN'] = np.sin(feature_df['CRS_ARR_DATETIME'].dt.hour/24*2*np.pi)\n",
    "feature_df['CRS_ARR_HOUR_COS'] = np.cos(feature_df['CRS_ARR_DATETIME'].dt.hour/24*2*np.pi)\n",
    "feature_df['CRS_DEP_HOUR_SIN'] = np.sin(feature_df['CRS_DEP_DATETIME'].dt.hour/24*2*np.pi)\n",
    "feature_df['CRS_DEP_HOUR_COS'] = np.cos(feature_df['CRS_DEP_DATETIME'].dt.hour/24*2*np.pi)\n",
    "dt = feature_df[\"CRS_DEP_DATETIME\"].dt\n",
    "time_of_day = 2*np.pi*(dt.hour*60+dt.minute)/3600\n",
    "feature_df['CRS_DEP_TOD_SIN'] = np.sin(time_of_day)\n",
    "feature_df['CRS_DEP_TOD_COS'] = np.cos(time_of_day)\n",
    "feature_df = feature_df.drop(['CRS_ARR_DATETIME','CRS_DEP_DATETIME','DEP_DELAY', 'Wind Dir_arr_SIN', 'Wind Dir_dep_COS'],axis=1)\n",
    "\n",
    "baseline_df = all_df[[\"CARRIER\",\"MONTH_SIN\",\"MONTH_COS\",\"DAY_OF_WEEK_SIN\",\"DAY_OF_WEEK_COS\",\"CRS_ARR_DATETIME\",\"CRS_DEP_DATETIME\",\"DEST\",\"ORIGIN\"]]\n",
    "\n",
    "baseline_df = pd.get_dummies(baseline_df)\n",
    "# norm_list_baseline = ['DEP_DELAY']\n",
    "# baseline_df = normalize(norm_list_baseline,baseline_df)\n",
    "baseline_df['CRS_ARR_HOUR_SIN'] = np.sin(baseline_df['CRS_ARR_DATETIME'].dt.hour/24*2*np.pi)\n",
    "baseline_df['CRS_ARR_HOUR_COS'] = np.cos(baseline_df['CRS_ARR_DATETIME'].dt.hour/24*2*np.pi)\n",
    "baseline_df['CRS_DEP_HOUR_SIN'] = np.sin(baseline_df['CRS_DEP_DATETIME'].dt.hour/24*2*np.pi)\n",
    "baseline_df['CRS_DEP_HOUR_COS'] = np.cos(baseline_df['CRS_DEP_DATETIME'].dt.hour/24*2*np.pi)\n",
    "baseline_df = baseline_df.drop(['CRS_ARR_DATETIME','CRS_DEP_DATETIME'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_df.columns)\n",
    "print(baseline_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(y_df[y_df > 0].size)\n",
    "print(y_df.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "# train baseline\n",
    "X_baseline = baseline_df.values\n",
    "y_binary = (y_df > 0).astype(int)\n",
    "X_baseline, X_test_baseline, y_baseline, y_test_baseline = train_test_split(X_baseline, y_binary, test_size=0.1, random_state=42)\n",
    "# X_baseline, X_val_baseline, y_baseline, y_val_baseline = train_test_split(X_total_baseline, y_total_baseline, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_binary[y_binary==0].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC(random_state=0)\n",
    "clf.fit(X_baseline, y_baseline)\n",
    "pred_train_baseline = clf.predict(X_baseline)\n",
    "pred_test_baseline = clf.predict(X_test_baseline)\n",
    "\n",
    "acc_train_baseline = accuracy_score(y_baseline, pred_train_baseline)\n",
    "acc_test_baseline = accuracy_score(y_test_baseline, pred_test_baseline)\n",
    "\n",
    "prec_train_baseline = precision_score(y_baseline, pred_train_baseline)\n",
    "prec_test_baseline = precision_score(y_test_baseline, pred_test_baseline)\n",
    "\n",
    "F1_train_baseline = f1_score(y_baseline, pred_train_baseline)\n",
    "F1_test_baseline = f1_score(y_test_baseline, pred_test_baseline)\n",
    "\n",
    "print (\"acc_train = \",acc_train_baseline)\n",
    "print(\"acc_test = \",acc_test_baseline)\n",
    "\n",
    "print('===============================')\n",
    "print (\"prec_train = \",prec_train_baseline)\n",
    "print(\"prec_test = \",prec_test_baseline)\n",
    "\n",
    "print('===============================')\n",
    "print (\"f1_train = \",F1_train_baseline)\n",
    "print(\"f1_test = \",F1_test_baseline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_binary = feature_df.values\n",
    "y_binary = (y_df > 0).astype(int)\n",
    "X, X_test_binary, y, y_test_binary = train_test_split(X_binary, y_binary, test_size=0.1, random_state=42)\n",
    "# X, X_val_binary, y, y_val_binary = train_test_split(X_total_binary, y_total_binary, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression predict binary\n",
    "clf_binary = LogisticRegressionCV(random_state=0)\n",
    "clf_binary.fit(X, y)\n",
    "pred_train_binary = clf_binary.predict(X)\n",
    "pred_test_binary = clf_binary.predict(X_test_binary)\n",
    "\n",
    "\n",
    "acc_train_binary = accuracy_score(y, pred_train_binary)\n",
    "acc_test_binary = accuracy_score(y_test_binary, pred_test_binary)\n",
    "\n",
    "prec_train_binary = precision_score(y, pred_train_binary)\n",
    "prec_test_binary = precision_score(y_test_binary, pred_test_binary)\n",
    "\n",
    "F1_train_binary = f1_score(y, pred_train_binary)\n",
    "F1_test_binary = f1_score(y_test_binary, pred_test_binary)\n",
    "\n",
    "print (\"acc_train = \",acc_train_binary)\n",
    "print(\"acc_test = \",acc_test_binary)\n",
    "\n",
    "print('===============================')\n",
    "print (\"prec_train = \",prec_train_binary)\n",
    "print(\"prec_test = \",prec_test_binary)\n",
    "\n",
    "print('===============================')\n",
    "print (\"f1_train = \",F1_train_binary)\n",
    "print(\"f1_test = \",F1_test_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVM predict binary\n",
    "clf_binary = LinearSVC(random_state=0)\n",
    "clf_binary.fit(X, y)\n",
    "pred_train_binary = clf_binary.predict(X)\n",
    "pred_test_binary = clf_binary.predict(X_test_binary)\n",
    "\n",
    "acc_train_binary = accuracy_score(y, pred_train_binary)\n",
    "acc_test_binary = accuracy_score(y_test_binary, pred_test_binary)\n",
    "\n",
    "prec_train_binary = precision_score(y, pred_train_binary)\n",
    "prec_test_binary = precision_score(y_test_binary, pred_test_binary)\n",
    "\n",
    "F1_train_binary = f1_score(y, pred_train_binary)\n",
    "F1_test_binary = f1_score(y_test_binary, pred_test_binary)\n",
    "\n",
    "print (\"acc_train = \",acc_train_binary)\n",
    "print(\"acc_test = \",acc_test_binary)\n",
    "\n",
    "print('===============================')\n",
    "print (\"prec_train = \",prec_train_binary)\n",
    "print(\"prec_test = \",prec_test_binary)\n",
    "\n",
    "print('===============================')\n",
    "print (\"f1_train = \",F1_train_binary)\n",
    "print(\"f1_test = \",F1_test_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SVM with poly kernel predict binary\n",
    "# clf_binary = SVC(random_state=0, kernel=\"poly\", degree=2)\n",
    "# clf_binary.fit(X, y)\n",
    "# pred_train_binary = clf_binary.predict(X)\n",
    "# pred_test_binary = clf_binary.predict(X_test_binary)\n",
    "\n",
    "# acc_train_binary = accuracy_score(y, pred_train_binary)\n",
    "# acc_test_binary = accuracy_score(y_test_binary, pred_test_binary)\n",
    "\n",
    "# prec_train_binary = precision_score(y, pred_train_binary)\n",
    "# prec_test_binary = precision_score(y_test_binary, pred_test_binary)\n",
    "\n",
    "# F1_train_binary = f1_score(y, pred_train_binary)\n",
    "# F1_test_binary = f1_score(y_test_binary, pred_test_binary)\n",
    "\n",
    "# print(\"poly SVM binary\")\n",
    "# print(\"acc_train = \",acc_train_binary)\n",
    "# print(\"acc_test = \",acc_test_binary)\n",
    "\n",
    "# print('===============================')\n",
    "# print(\"prec_train = \",prec_train_binary)\n",
    "# print(\"prec_test = \",prec_test_binary)\n",
    "\n",
    "# print('===============================')\n",
    "# print(\"f1_train = \",F1_train_binary)\n",
    "# print(\"f1_test = \",F1_test_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict multiclass\n",
    "X = feature_df.values\n",
    "X_baseline = baseline_df.values\n",
    "y_raw = (y_df.values/15).astype(int)\n",
    "X, X_test, y, y_test = train_test_split(X, y_raw, test_size=0.1, random_state=42)\n",
    "X_baseline, X_baseline_test, y_baseline, y_test = train_test_split(X_baseline, y_raw, test_size=0.1, random_state=42)\n",
    "# X, X_val, y, y_val = train_test_split(X_total, y_total, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM multiclass baseline\n",
    "clf = LinearSVC(random_state=0)\n",
    "clf.fit(X_baseline,  y_baseline)\n",
    "\n",
    "pred_train = clf.predict(X_baseline)\n",
    "pred_test = clf.predict(X_baseline_test)\n",
    "\n",
    "acc_train_multi = accuracy_score(y_baseline, pred_train)\n",
    "acc_test_multi = accuracy_score(y_test, pred_test)\n",
    "\n",
    "F1_train_multi = f1_score(y_baseline, pred_train, average = 'weighted', labels=np.unique(pred_train))\n",
    "F1_test_multi = f1_score(y_test, pred_test, average = 'weighted', labels=np.unique(pred_test))\n",
    "\n",
    "print (\"SVM multiclass baseline\")\n",
    "print (\"acc_train = \",acc_train_multi)\n",
    "print(\"acc_test = \",acc_test_multi)\n",
    "print('===============================')\n",
    "print (\"f1_train = \",F1_train_multi)\n",
    "print(\"f1_test = \",F1_test_multi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM predict multiclass\n",
    "clf = LinearSVC(random_state=0)\n",
    "clf.fit(X,  y)\n",
    "\n",
    "pred_train = clf.predict(X)\n",
    "# pred_val = clf.predict(X_val)\n",
    "pred_test = clf.predict(X_test)\n",
    "\n",
    "acc_train_multi = accuracy_score(y, pred_train)\n",
    "# acc_val_multi = accuracy_score(y_val, pred_val)\n",
    "acc_test_multi = accuracy_score(y_test, pred_test)\n",
    "\n",
    "F1_train_multi = f1_score(y, pred_train, average = 'weighted', labels=np.unique(pred_train))\n",
    "# F1_val_multi = f1_score(y_val, pred_val, average = 'weighted', labels=np.unique(pred_val))\n",
    "F1_test_multi = f1_score(y_test, pred_test, average = 'weighted', labels=np.unique(pred_test))\n",
    "\n",
    "print (\"acc_train = \",acc_train_multi)\n",
    "# print (\"acc_val = \",acc_val_multi)\n",
    "print(\"acc_test = \",acc_test_multi)\n",
    "print('===============================')\n",
    "print (\"f1_train = \",F1_train_multi)\n",
    "# print (\"f1_val = \",F1_val_multi)\n",
    "print(\"f1_test = \",F1_test_multi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LogisticRegression predict multiclass\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(X,  y)\n",
    "\n",
    "pred_train = clf.predict(X)\n",
    "# pred_val = clf.predict(X_val)\n",
    "pred_test = clf.predict(X_test)\n",
    "\n",
    "acc_train_multi = accuracy_score(y, pred_train)\n",
    "# acc_val_multi = accuracy_score(y_val, pred_val)\n",
    "acc_test_multi = accuracy_score(y_test, pred_test)\n",
    "\n",
    "F1_train_multi = f1_score(y, pred_train, average = 'weighted', labels=np.unique(pred_train))\n",
    "# F1_val_multi = f1_score(y_val_baseline, pred_val_baseline, average = 'weighted', labels=np.unique(pred_val_baseline))\n",
    "F1_test_multi = f1_score(y_test, pred_test, average = 'weighted', labels=np.unique(pred_test))\n",
    "\n",
    "print (\"acc_train = \",acc_train_multi)\n",
    "# print (\"acc_val = \",acc_val_multi)\n",
    "print(\"acc_test = \",acc_test_multi)\n",
    "print('===============================')\n",
    "print (\"f1_train = \",F1_train_multi)\n",
    "# print (\"f1_val = \",F1_val_multi)\n",
    "print(\"f1_test = \",F1_test_multi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_rand = RandomForestClassifier(max_depth=10, random_state=0)\n",
    "clf_rand.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rand\n",
    "pred_train_rand = clf_rand.predict(X)\n",
    "pred_val_rand = clf_rand.predict(X_val)\n",
    "\n",
    "acc_train_rand = accuracy_score(y, pred_train_rand)\n",
    "acc_val_rand = accuracy_score(y_val, pred_val_rand)\n",
    "print (\"acc_train_rand = \",acc_train_rand)\n",
    "print (\"acc_val_rand = \",acc_val_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
