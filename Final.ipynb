{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements\n",
    "```\n",
    "pip3 install pandas lxml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching data from BTS database\n",
    "\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "def get_data_csv():\n",
    "    years_interval = [\"2015\",\"2018\"]\n",
    "    req_header = {\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Accept-Language\": \"zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "        \"Cache-Control\": \"no-cache\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "        \"Cookie\": \"ASPSESSIONIDCQCDRTCQ=FKGMOCEDMCDHINELAEGCEIKN\",\n",
    "        \"Host\": \"www.transtats.bts.gov\",\n",
    "        \"Origin\": \"https://www.transtats.bts.gov\",\n",
    "        \"Pragma\": \"no-cache\",\n",
    "        \"Referer\": \"https://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&DB_Short_Name=On-Time\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.139 Safari/537.36\"\n",
    "    }\n",
    "    data_url = \"https://www.transtats.bts.gov/DownLoad_Table.asp?Table_ID=236&Has_Group=3&Is_Zipped=0\"\n",
    "    pair_form_data = {\n",
    "    \"UserTableName\":\"On_Time_Performance\",\n",
    "    \"DBShortName\":\"On_Time\",\n",
    "    \"RawDataTable\":\"T_ONTIME\",\n",
    "    \"sqlstr\":\\\n",
    "            \"SELECT YEAR,QUARTER,MONTH,DAY_OF_MONTH,DAY_OF_WEEK,FL_DATE,CARRIER,TAIL_NUM,ORIGIN,DEST,CRS_DEP_TIME,DEP_TIME,DEP_DELAY,CRS_ARR_TIME,ARR_TIME,ARR_DELAY,ARR_DELAY_GROUP,DISTANCE,CANCELLED,DIVERTED,CARRIER_DELAY,WEATHER_DELAY,NAS_DELAY,SECURITY_DELAY,LATE_AIRCRAFT_DELAY \\\n",
    "            FROM T_ONTIME \\\n",
    "            WHERE (\\\n",
    "                DEST IN ('EWR','JFK','LGA') OR\\\n",
    "                ORIGIN IN ('EWR','JFK','LGA')\\\n",
    "            )\\\n",
    "            AND \\\n",
    "            Month BETWEEN 1 AND 12 AND \\\n",
    "            YEAR BETWEEN {} AND {}\".format(years_interval[0],years_interval[1]),\n",
    "    \"varlist\":\"YEAR,QUARTER,MONTH,DAY_OF_MONTH,DAY_OF_WEEK,FL_DATE,CARRIER,TAIL_NUM,ORIGIN,DEST,CRS_DEP_TIME,DEP_TIME,DEP_DELAY,CRS_ARR_TIME,ARR_TIME,ARR_DELAY,ARR_DELAY_GROUP,DISTANCE,CANCELLED,DIVERTED,CARRIER_DELAY,WEATHER_DELAY,NAS_DELAY,SECURITY_DELAY,LATE_AIRCRAFT_DELAY\",\n",
    "    }\n",
    "    r = requests.post(data_url,\n",
    "                      data=pair_form_data,\n",
    "                      headers=req_header,\n",
    "                     )\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    return pd.read_csv(io.BytesIO(z.read(z.infolist()[0])),\n",
    "                       dtype={\n",
    "                          \"CRS_DEP_TIME\":str,\n",
    "                          \"DEP_TIME\":str,\n",
    "                          \"CRS_ARR_TIME\":str,\n",
    "                          \"ARR_TIME\":str,\n",
    "                          \"CANCELLED\":bool,\n",
    "                          \"DIVERTED\":bool},\n",
    "                      usecols=list(range(24))\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping hourly weather data from wunderground.com\n",
    "\n",
    "import lxml\n",
    "from multiprocessing.dummy import Pool\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "NYC_AIRPORTS = [\"EWR\",\"JFK\",\"LGA\"]\n",
    "MIA_AIRPORTS = [\"FLL\",\"MIA\"]\n",
    "\n",
    "def wind_speed_conv(x):\n",
    "    try:\n",
    "        ret = 0.0 if x == \"Calm\" else float(x)\n",
    "    except:\n",
    "        ret = np.NaN\n",
    "    return ret\n",
    "DIR_ANGLE={\n",
    "            \"North\": 0,\"NNE\": 1,\"NE\": 2,\"ENE\": 3,\"East\": 4,\"ESE\": 5,\"SE\": 6,\"SSE\": 7,\n",
    "            \"South\": 8,\"SSW\": 9,\"SW\":10,\"WSW\":11,\"West\":12,\"WNW\":13,\"NW\":14,\"NNW\":15,\n",
    "          }\n",
    "def wind_dir_conv(x):\n",
    "    return np.NaN if x not in DIR_ANGLE else 22.5*DIR_ANGLE[x]\n",
    "def humidity_conv(x):\n",
    "    try:\n",
    "        ret = float(x.strip(\"%\"))/100\n",
    "    except:\n",
    "        ret = np.NaN\n",
    "    return ret\n",
    "def precip_conv(x):\n",
    "    try:\n",
    "        ret = 0.0 if x == \"N/A\" else float(x)\n",
    "    except:\n",
    "        ret = 0.0\n",
    "    return ret\n",
    "def event_conv(x):\n",
    "    return \"\".join(x.split(\"\\t\"))\n",
    "\n",
    "\n",
    "def table_parser(pair):\n",
    "    url, date, airport = pair\n",
    "    content = requests.get(url).content\n",
    "    def time_conv(x):\n",
    "        return date + \" \" + x\n",
    "    bs = BeautifulSoup(content, \"html5lib\")\n",
    "    table_html = bs.find(id=\"obsTable\")\n",
    "    for unit_tag in table_html.find_all(\"span\",class_=\"wx-unit\"):\n",
    "        unit_tag.decompose()\n",
    "    table_df = pd.read_html(str(table_html), converters= \\\n",
    "                            {\n",
    "                                \"Time (EST)\":time_conv,\n",
    "                                \"Time (EDT)\":time_conv,\n",
    "                                \"Wind Speed\":wind_speed_conv,\n",
    "                                \"Wind Dir\":wind_dir_conv,\n",
    "                                \"Precip\":precip_conv,\n",
    "                                \"Humidity\":humidity_conv,\n",
    "                                \"Events\":event_conv\n",
    "                            })[0]\n",
    "    table_df = table_df.drop([\"Windchill\", \"Dew Point\", \"Gust Speed\"],axis=1, errors=\"ignore\")\n",
    "    table_df.rename(columns={\"Time (EST)\":\"Time\", \"Time (EDT)\":\"Time\", \"Temp.\":\"Temp\"}, inplace=True)\n",
    "    table_df[\"Time\"] = pd.to_datetime(table_df[\"Time\"])\n",
    "    table_df.set_index(\"Time\", inplace=True)\n",
    "    table_df[\"Airport\"] = airport\n",
    "    return table_df\n",
    "\n",
    "def weather_scraper(dates):\n",
    "    # Run once. Slow to run.\n",
    "    base_url = \"https://www.wunderground.com/history/airport/K{}/{}/DailyHistory.html\"\n",
    "    \n",
    "    pool = Pool(8)\n",
    "    pairs = []\n",
    "    for airport in NYC_AIRPORTS + MIA_AIRPORTS:\n",
    "        for date in dates:\n",
    "            date_str = date.strftime(\"%Y/%m/%d\")\n",
    "            url = base_url.format(airport, date_str)\n",
    "            pairs.append((url, date_str, airport))\n",
    "            \n",
    "    weather_df_list = pool.map(table_parser, pairs)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "            \n",
    "    result = pd.concat(weather_df_list)\n",
    "    result.to_csv(\"weather.csv\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv(\"weather.csv\", low_memory=False, parse_dates=[\"Time\"], na_values=[\"-\"], index_col=\"Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle weather with DST\n",
    "new_weather_list = []\n",
    "for a in NYC_AIRPORTS+MIA_AIRPORTS:\n",
    "    weather_airport_idx = weather[weather[\"Airport\"]==a].index\n",
    "    weather_airport_tz_idx = weather_airport_idx.tz_localize('US/Eastern', ambiguous='infer')\n",
    "    new_weather = weather[weather[\"Airport\"]==a].set_index(weather_airport_tz_idx)\n",
    "    new_weather_list.append(new_weather[~new_weather.index.duplicated()])\n",
    "new_weather = pd.concat(new_weather_list)\n",
    "    \n",
    "new_weather.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = get_data_csv()\n",
    "raw_data.to_csv(\"data.csv\", index=False)\n",
    "raw_data = pd.read_csv(\"data.csv\",dtype={\n",
    "                          \"CRS_DEP_TIME\":str,\n",
    "                          \"DEP_TIME\":str,\n",
    "                          \"CRS_ARR_TIME\":str,\n",
    "                          \"ARR_TIME\":str,\n",
    "                          \"CANCELLED\":bool,\n",
    "                          \"DIVERTED\":bool})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data[[\"DEP_DELAY\",\"ARR_DELAY\"]] = raw_data[[\"DEP_DELAY\",\"ARR_DELAY\"]].fillna(0)\n",
    "data = raw_data[~(raw_data[\"DIVERTED\"] | raw_data[\"CANCELLED\"])].drop([\"CANCELLED\",\"DIVERTED\"], axis=1)\n",
    "\n",
    "time_cols = [\"CRS_DEP_TIME\", \"CRS_ARR_TIME\"]\n",
    "for col in time_cols:\n",
    "    data.loc[data[col]==\"2400\",col] = \"0000\"\n",
    "    data[col.rsplit(\"_\",1)[0]+\"_DATETIME\"] = \\\n",
    "    pd.to_datetime(data[\"FL_DATE\"]+\" \"+data[col], format=\"%Y-%m-%d %H%M\")\n",
    "    \n",
    "# check if arrival time is after departure\n",
    "data.loc[(data['CRS_ARR_DATETIME'] < data['CRS_DEP_DATETIME']), 'CRS_ARR_DATETIME'] += pd.Timedelta(days=1)\n",
    "data['CRS_DEP_DATETIME'] = pd.DatetimeIndex(data[\"CRS_DEP_DATETIME\"]).tz_localize(\"US/Eastern\", ambiguous=False, errors=\"coerce\")\n",
    "data['CRS_ARR_DATETIME'] = pd.DatetimeIndex(data[\"CRS_ARR_DATETIME\"]).tz_localize(\"US/Eastern\", ambiguous=False, errors=\"coerce\")\n",
    "\n",
    "data[\"DEP_DATETIME\"] = data[\"CRS_DEP_DATETIME\"] + pd.to_timedelta(data[\"DEP_DELAY\"], unit=\"m\")\n",
    "data[\"ARR_DATETIME\"] = data[\"CRS_ARR_DATETIME\"] + pd.to_timedelta(data[\"ARR_DELAY\"], unit=\"m\")\n",
    "data = data[~(data[[\"DEP_DATETIME\",\"ARR_DATETIME\"]].isnull().any(axis=1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "mia_data = data.drop(time_cols+[\"DEP_TIME\", \"ARR_TIME\"],axis=1)\n",
    "mia_data = mia_data[mia_data[\"DEST\"].isin(MIA_AIRPORTS)]# | mia_data[\"ORIGIN\"].isin(MIA_AIRPORTS)]\n",
    "mia_data[\"date\"] = pd.to_datetime(mia_data[\"FL_DATE\"])\n",
    "mia_data = mia_data.set_index(\"date\").sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather_scraper(mia_data.index.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.merge_asof(mia_data.sort_values(\"CRS_DEP_DATETIME\"), new_weather, left_by=[\"ORIGIN\"], right_by=[\"Airport\"], left_on=\"CRS_DEP_DATETIME\", right_index=True)\n",
    "weather_flight_data = pd.merge_asof(temp_df.sort_values(\"CRS_ARR_DATETIME\"), new_weather, left_by=[\"DEST\"], right_by=[\"Airport\"], left_on=\"CRS_ARR_DATETIME\", right_index=True, suffixes=[\"_dep\",\"_arr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_flight_data = pd.merge_asof(weather_flight_data.sort_values(\"CRS_DEP_DATETIME\"),\n",
    "                                    data[[\"ARR_DATETIME\",\"TAIL_NUM\",\"DEST\",\"ARR_DELAY\"]].sort_values(\"ARR_DATETIME\"),\n",
    "                                    left_by=[\"TAIL_NUM\",\"ORIGIN\"],\n",
    "                                    right_by=[\"TAIL_NUM\",\"DEST\"],\n",
    "                                    left_on=\"CRS_DEP_DATETIME\",\n",
    "                                    right_on=\"ARR_DATETIME\",\n",
    "                                    suffixes=(\"\",\"_PREV\"),\n",
    "                                    allow_exact_matches=False)\n",
    "weather_flight_data[\"PREV_ARR_LT_ONE_DAY\"] = (weather_flight_data[\"CRS_DEP_DATETIME\"] - weather_flight_data[\"ARR_DATETIME_PREV\"] < pd.Timedelta(days=1)).astype(int)\n",
    "weather_flight_data.drop([\"ARR_DATETIME_PREV\", \"DEST_PREV\"],axis=1, inplace=True)\n",
    "weather_flight_data[\"ARR_DELAY_PREV\"] = weather_flight_data[\"ARR_DELAY_PREV\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86325, 46)"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "#['Month','Carrier','origin','Dest','distance'] \n",
    "total_table = data[['MONTH','CARRIER','ORIGIN','DEST','DISTANCE','DEP_DELAY_GROUP']]\n",
    "#y_table = data['DEP_DELAY_GROUP'].dropna(axis=0, how='any')\n",
    "total_table = total_table.dropna(axis=0, how='any')\n",
    "total_table = total_table[total_table.DEP_DELAY_GROUP >0]\n",
    "y_table = total_table['DEP_DELAY_GROUP']\n",
    "y_table = y_table.reset_index(drop=True)\n",
    "feature_table = total_table.drop(['DEP_DELAY_GROUP'],axis=1)\n",
    "feature_table = feature_table.reset_index(drop=True)\n",
    "\n",
    "\n",
    "feature_table['MONTH']= feature_table['MONTH'].astype('category')\n",
    "feature_table = pd.get_dummies(feature_table)\n",
    "feature_table.head()\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(feature_table[['DISTANCE']].values.astype(float))\n",
    "#feature_table['DISTANCE']= \n",
    "feature_table['DISTANCE']= pd.DataFrame(x_scaled)\n",
    "feature_table.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_table = data['DEP_DELAY_GROUP']\n",
    "#y_table = pd.get_dummies(y_table)\n",
    "y_table.head()\n",
    "#print (y_table.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC(random_state=0)\n",
    "print (feature_table.values)\n",
    "\n",
    "clf.fit(feature_table.values, y_table.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = clf.predict(feature_table.values)\n",
    "count_predict =0\n",
    "for i in prediction:\n",
    "    if i != 1:\n",
    "        print (i)\n",
    "        count_predict+=1\n",
    "\n",
    "count = 0\n",
    "for i in y_table.values:\n",
    "    if i !=1:\n",
    "        count+=1\n",
    "        #print (i)\n",
    "print ('count = ',count)\n",
    "print (\"count_predict =\",count_predict)\n",
    "print (y_table.shape)\n",
    "print (prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
